{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243871ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ast\n",
    "import copy\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27686bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS \n",
    "SUMMERIES_PATH = \"./GP_RESULTS_SUMMERIES\"\n",
    "METRICS = 'TPR_TNR'\n",
    "PROJECT = \"rq1_results\"\n",
    "EXP_ID = 'default'\n",
    "ML_DATA_PATH = './ML_RES_predcr'\n",
    "ORGS = ['Eclipse','Libreoffice','Gerrithub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers\n",
    "def GET_DATA() :\n",
    "    return pd.read_csv(os.path.join(SUMMERIES_PATH,f'GP_{PROJECT}_{METRICS}_{EXP_ID}.csv'))\n",
    "#creating the select_subset selection function to select the appropriate data\n",
    "\n",
    "def get_ML_data(path,project,model_name) : \n",
    "    return pd.read_csv(os.path.join(ML_RESULTS_PATH,f'{project}_{model_name}_test_result_cross.csv'))\n",
    "\n",
    "def prepare_ML_DATA(ML_DATA_PATH) : \n",
    "    all_ML_DATA = pd.concat([\n",
    "        pd.read_csv(os.path.join(ML_DATA_PATH,'Eclipse_all_models.csv')),\n",
    "        pd.read_csv(os.path.join(ML_DATA_PATH,'Libreoffice_all_models.csv')),\n",
    "        pd.read_csv(os.path.join(ML_DATA_PATH,'Gerrithub_all_models.csv'))\n",
    "\n",
    "    ])\n",
    "    final_ml_data = process_ml_data(all_ML_DATA)\n",
    "    return final_ml_data\n",
    "\n",
    "def prepare_GP_results(GP_DATA_PATH) : \n",
    "    all_GP_data = pd.concat([\n",
    "        pd.read_csv(os.path.join(GP_DATA_PATH,'MULTICR_performance.csv')),\n",
    "        pd.read_csv(os.path.join(GP_DATA_PATH,'other_moeas.csv'))\n",
    "    ])\n",
    "    all_GP_data = all_GP_data.rename(columns={\"tnr\": \"recall_M\", \"tpr\": \"recall_A\",'f1': 'f1_A','precision':\"precision_A\"})\n",
    "    all_GP_data = add_metric(all_GP_data,'precision_M',precision_M)\n",
    "    all_GP_data = add_metric(all_GP_data,'f1_M',f1_M)\n",
    "    all_GP_data.loc[all_GP_data[\"project_name\"] == \"eclipse\",'project_name'] = 'Eclipse'\n",
    "    all_GP_data['file_id'] = all_GP_data.apply(lambda row: captilize_first_letter(row['project_name']) + '_' + str(row['fold'] ),axis=1)\n",
    "    \n",
    "    all_GP_data = select_all_but_best(all_GP_data)\n",
    "    return all_GP_data\n",
    "\n",
    "def process_ml_data(ml_data) : \n",
    "    result = ml_data.copy()\n",
    "    result = result.rename(columns={\"recall_m\": \"recall_M\", \"recall_a\": \"recall_A\",\n",
    "                                    'f1_score_a': 'f1_A','precision_a':\"precision_A\",\n",
    "                                    'f1_score_m' : 'f1_M','precision_m' : 'precision_M',\n",
    "                                    'project': 'project_name'\n",
    "                                   })\n",
    "    result[\"fold\"] = result['fold'] - 1 \n",
    "    result['file_id'] = result.apply(lambda row: row['project_name'] + '_' + str(row['fold'] ),axis=1)\n",
    "    \n",
    "    return result \n",
    "def select_subset(data,project_name,algorithm_name,file_id =None) : \n",
    "    if file_id != None : \n",
    "        selected_data = data.loc[(data[\"projet_name\"] == project_name) & (data[\"algorithm\"] == algorithm_name) & (data[\"file_id\"] == file_id)]\n",
    "    else : \n",
    "        selected_data = data.loc[(data[\"projet_name\"] == project_name) & (data[\"algorithm\"] == algorithm_name)]\n",
    "   \n",
    "    return selected_data\n",
    "\n",
    "def select_by_model(data,models_ids) : \n",
    "    selected_data = data.loc[data[\"model_id\"].isin(models_ids)] \n",
    "    return selected_data\n",
    "\n",
    "def select_by_run(data,runs) : \n",
    "    selected_data = data.loc[data[\"run\"].isin(runs)] \n",
    "    return selected_data\n",
    "    \n",
    "def select_by_project_list(data,projects) : \n",
    "    selected_data = data.loc[data[\"projet_name\"].isin(projects)]\n",
    "    return selected_data\n",
    "\n",
    "def select_by_filenames(data,files) : \n",
    "    selected_data = data.loc[data[\"file_id\"].isin(files)]\n",
    "    return selected_data\n",
    "\n",
    "def select_algorithms(data,selected_algos) : \n",
    "    selected_data = data.loc[data[\"algorithm\"].isin(selected_algos)]\n",
    "    return selected_data\n",
    "\n",
    "def select_only_best_model(data) : \n",
    "    selected_data = data.loc[(data[\"model_id\"] == \"best_model_performance\")]\n",
    "    return selected_data\n",
    "\n",
    "def select_all_but_best(data) : \n",
    "    selected_data = data.loc[(data[\"model_id\"] != \"best_model_performance\")]\n",
    "    return selected_data\n",
    "\n",
    "def select_train_or_test(data,train_or_test) : \n",
    "    selected_data = data.loc[(data[\"train_or_test\"] == train_or_test)]\n",
    "    return selected_data\n",
    "\n",
    "def add_metrics_product(data,metrics): \n",
    "    result = data.copy()\n",
    "    def lambda_fun(row,metrics) :\n",
    "        res = 1.0 \n",
    "        for metric in metrics: \n",
    "            res *= row[metric]\n",
    "        return res\n",
    "    col_name = str(metrics[0])\n",
    "    for col in metrics[1:] : \n",
    "        col_name+= \"_times_\"+col\n",
    "    result[col_name] = result.apply (lambda row: lambda_fun(row,metrics), axis=1)\n",
    "    return result \n",
    "\n",
    "def add_project_name(data): \n",
    "    result = data.copy()\n",
    "    result[\"project_name\"] = result.apply (lambda row: PROJECT, axis=1)\n",
    "    return result \n",
    "\n",
    "def generate_project_name(data) : \n",
    "    result = data.copy()\n",
    "    result[\"project_name\"] = result.apply (lambda row: row['file_id'].split('_')[0], axis=1)\n",
    "    return result \n",
    "\n",
    "def add_metric(data,metric_name,metric_func): \n",
    "    data_copy=data.copy()\n",
    "    data_copy[metric_name] = data_copy.apply (lambda row: metric_func(row), axis=1)\n",
    "    return data_copy \n",
    "\n",
    "def find_best_model(data,criteria,max_is_better=True) :\n",
    "    data_copy = data.copy()\n",
    "    data_copy = data_copy.reset_index()\n",
    "    criteria_col = data_copy[criteria]\n",
    "    \n",
    "    if max_is_better: \n",
    "        best_index = criteria_col.idxmax()\n",
    "    else :\n",
    "        best_index = criteria_col.idxmin()\n",
    "    return data_copy.loc[best_index,'model_id']\n",
    "\n",
    "def apply_selection_criteria(data,criteria,metrics = ['f1','G','MCC','tpr','tnr','precision'],max_is_better = True): \n",
    "    file_ids = data['file_id'].unique()\n",
    "    runs = data[\"run\"].unique()\n",
    "    algos = data['algorithm'].unique()\n",
    "    train_data =  select_train_or_test(data,\"train\")\n",
    "    test_data =  select_train_or_test(data,\"test\")\n",
    "    #train_data = train_data[(train_data['recall_A'] > 0.8) &(train_data['recall_M'] > 0.8)  ]\n",
    "    #print(len(train_data))\n",
    "    print(file_ids)\n",
    "    result = []\n",
    "    for file_id in file_ids:\n",
    "        for run in runs :\n",
    "            for algo in algos: \n",
    "                file_id_train = select_all_but_best(select_by_run(select_algorithms(select_by_filenames(train_data,[file_id]),[algo]),[run]))\n",
    "                #print(file_id_train)\n",
    "                file_id_test = select_all_but_best(select_by_run(select_algorithms(select_by_filenames(test_data,[file_id]),[algo]),[run]))\n",
    "                #print(file_id_test)\n",
    "                try:\n",
    "                    best_model = find_best_model(file_id_train,criteria,max_is_better)\n",
    "                except : \n",
    "                    continue \n",
    "                print(best_model)\n",
    "                new_row = {\n",
    "                    'file_id':file_id,\n",
    "                    'run' : run,\n",
    "                    'algorithm' : algo,\n",
    "                    'model_id':best_model\n",
    "                }\n",
    "                print(\"=====================================================================================================\")\n",
    "                print(file_id_test[file_id_test['model_id'] == best_model][metrics])\n",
    "                print('len selected:',len(file_id_test[file_id_test['model_id'] == best_model][metrics]))\n",
    "                print(\"=====================================================================================================\")\n",
    "                model_performance = list(file_id_test[file_id_test['model_id'] == best_model][metrics].to_dict(orient = 'index').values())[0]\n",
    "                print(model_performance)\n",
    "                new_row.update(model_performance)\n",
    "                result.append(new_row)\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "#new_metrics \n",
    "def precision_M(row) :\n",
    "    if (row['tn'] + row['fn']) == 0 :\n",
    "        return 1\n",
    "    return row['tn']/(row['tn'] + row['fn'])\n",
    "\n",
    "def f1_M(row) : \n",
    "    if 'precision_M' in row: \n",
    "        prec_M = row['precision_M']\n",
    "    else :\n",
    "        prec_M = row['tn']/(row['tn'] + row['fn'])\n",
    "    \n",
    "    if 'tnr' in row: \n",
    "        rec_M = row['tnr']\n",
    "    elif 'recall_M' in row : \n",
    "        rec_M = row['recall_M']\n",
    "    else:\n",
    "        rec_M = row['tn']/(row['tn'] + row['fp'])\n",
    "    \n",
    "    if prec_M + rec_M == 0 :\n",
    "        return 1\n",
    "    return 2*prec_M*rec_M/(prec_M + rec_M)\n",
    "\n",
    "def captilize_first_letter(word) :\n",
    "    return word[0].upper() + word[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2: Within-project validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "GP_DATA = prepare_GP_results(GP_RESULTS_PATH)\n",
    "ML_DATA = prepare_ML_DATA(ML_DATA_PATH)\n",
    "print(GP_DATA.columns)\n",
    "#print(ML_DATA.columns)\n",
    "#print(ML_DATA['file_id'].unique())\n",
    "print(GP_DATA['file_id'].unique())\n",
    "STUDIED_COLUMNS = [ 'project_name','file_id','fold','run','algorithm','model_id','train_or_test','MCC','f1_A','f1_M',  'recall_A', 'recall_M',  'precision_A','precision_M']\n",
    "FINAL_DATA = pd.concat([\n",
    "    GP_DATA[STUDIED_COLUMNS], ML_DATA[STUDIED_COLUMNS]\n",
    "])\n",
    "FINAL_DATA = add_metrics_product(FINAL_DATA,['MCC','f1_M'])\n",
    "FINAL_DATA = add_metrics_product(FINAL_DATA,['MCC','f1_A'])\n",
    "\n",
    "FINAL_RESULT = apply_selection_criteria(FINAL_DATA,'MCC_times_f1_M',metrics = ['project_name','MCC','f1_M','f1_A', 'recall_A', 'recall_M',  'precision_A','precision_M'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34158b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCC_F1_G = add_metrics_product(new_data,['MCC','f1_A'])\n",
    "res = apply_selection_criteria(MCC_F1_G,'MCC_times_f1_A',metrics = ['MCC','G','f1_M','precision_M','recall_M','f1_A','precision_A','recall_A'])\n",
    "res = generate_project_name(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ebca6",
   "metadata": {},
   "source": [
    "## RQ3: Cross-project validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aef3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_CP_RESULTS_PATh = '../GP_summeries\\GP_RESULTS/cross_project'\n",
    "ML_CP_RESULTS_PATh = '../GP_summeries/ML_RESULTS/cross_project'\n",
    "ML_CP_1_RESULTS_PATh = os.path.join(ML_CP_RESULTS_PATh,'ML_scenario_1_results.csv')\n",
    "ML_CP_2_RESULTS_PATh = os.path.join(ML_CP_RESULTS_PATh,'ML_scenario_2_results.csv')\n",
    "GP_CP_1_RESULTS_PATh = os.path.join(GP_CP_RESULTS_PATh,'cp_scenario_1_multicr.csv')\n",
    "GP_CP_2_RESULTS_PATh = os.path.join(GP_CP_RESULTS_PATh,'cp_scenario_2_multicr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cp_1_ML_DATA(ML_DATA_PATH) : \n",
    "    all_data = pd.read_csv(ML_DATA_PATH)\n",
    "    all_data['project_name'] = all_data.apply(lambda row : row['Source'] + \"_\" + row['Target'],axis = 1)\n",
    "    return all_data\n",
    "\n",
    "def prepare_cp_1_GP_results(GP_DATA_PATH) : \n",
    "    all_data = pd.read_csv(GP_DATA_PATH)\n",
    "    all_data = all_data.rename(columns={\"tnr\": \"recall_M\", \"tpr\": \"recall_A\",'f1': 'f1_A','precision':\"precision_A\"})\n",
    "    all_data = add_metric(all_data,'precision_M',precision_M)\n",
    "    all_data = add_metric(all_data,'f1_M',f1_M)\n",
    "    all_data = add_metrics_product(all_data,['MCC','f1_M'])\n",
    "    all_data['project_name'] = all_data.apply(lambda row : row['Source'] + \"_\" + row['Target'],axis = 1 )\n",
    "    return all_data\n",
    "\n",
    "def prepare_cp_2_ML_DATA(ML_DATA_PATH) : \n",
    "    all_data = pd.read_csv(ML_DATA_PATH)\n",
    "    return all_data\n",
    "\n",
    "def prepare_cp_2_GP_results(GP_DATA_PATH) : \n",
    "    all_data = pd.read_csv(GP_DATA_PATH)\n",
    "    all_data = all_data.rename(columns={\"tnr\": \"recall_M\", \"tpr\": \"recall_A\",'f1': 'f1_A','precision':\"precision_A\"})\n",
    "    all_data = add_metric(all_data,'precision_M',precision_M)\n",
    "    all_data = add_metric(all_data,'f1_M',f1_M)\n",
    "    all_data = add_metrics_product(all_data,['MCC','f1_M'])\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745bff0",
   "metadata": {},
   "source": [
    "### Cross project scenario 1: One project source One target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_cp1_data = prepare_cp_1_ML_DATA(ML_CP_1_RESULTS_PATh)\n",
    "#print(ml_cp1_data.columns)\n",
    "gp_cp1_data = prepare_cp_1_GP_results(GP_CP_1_RESULTS_PATh)\n",
    "print(gp_cp1_data.columns)\n",
    "gp_cp1_data_selected = apply_selection_criteria(gp_cp1_data,'MCC_times_f1_M',metrics = ['Source', 'Target', 'run', 'algorithm', 'MCC', 'f1_A', 'f1_M',\n",
    "       'model_id','project_name'])\n",
    "print(gp_cp1_data)\n",
    "all_cp1_data = pd.concat([gp_cp1_data_selected[['Source', 'Target', 'run', 'algorithm', 'MCC', 'f1_A', 'f1_M',\n",
    "       'model_id','project_name']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea4652",
   "metadata": {},
   "source": [
    "### Cross project scenario 2: All projects source One target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df765099",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_cp2_data = prepare_cp_2_ML_DATA(ML_CP_2_RESULTS_PATh)\n",
    "print(ml_cp2_data.columns)\n",
    "gp_cp2_data = prepare_cp_2_GP_results(GP_CP_2_RESULTS_PATh)\n",
    "print(gp_cp2_data.columns)\n",
    "gp_cp2_data_selected = apply_selection_criteria(gp_cp2_data,'MCC_times_f1_M',metrics = ['project_name','run', 'algorithm', 'MCC', 'f1_A', 'f1_M',\n",
    "       'model_id','project_name'])\n",
    "gp_cp2_data\n",
    "all_cp2_data = pd.concat([ml_cp2_data[['project_name', 'algorithm', 'run', 'model_id', 'MCC', 'f1_M', 'f1_A']],gp_cp2_data_selected[['project_name', 'algorithm', 'run', 'model_id', 'MCC', 'f1_M', 'f1_A']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61d07e",
   "metadata": {},
   "source": [
    "## Models complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cv_data(path,org,run,model_name) : \n",
    "    return pickle.load(open(os.path.join(path,org,model_name,f'{org}_{model_name}_cross_val_run_{run}.pk'),'rb'))\n",
    "\n",
    "def compute_DT_max_depth(dt_model) : \n",
    "    return dt_model.tree_.max_depth \n",
    "\n",
    "def nodes_count(dt_model) : \n",
    "    return dt_model.tree_.node_count\n",
    "\n",
    "def compute_DT_model_complexity(dt_model, internal_node_complexity = 3, leaf_node_complexity = 1) :\n",
    "    complexity=0\n",
    "    children_left = dt_model.tree_.children_left\n",
    "    children_right = dt_model.tree_.children_right\n",
    "    for node_id in range(dt_model.tree_.node_count): \n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        if is_split_node: \n",
    "            complexity += internal_node_complexity \n",
    "        else : \n",
    "            complexity += leaf_node_complexity\n",
    "\n",
    "    return complexity\n",
    "\n",
    "def compute_dt_models_complexities(orgs,runs,data_path,folds=range(10)) : \n",
    "    results = []\n",
    "    for org in orgs: \n",
    "        for fold in folds: \n",
    "            for run in runs: \n",
    "                new_row = {'project_name' : org,'fold' : fold, 'run' : run,'algorithm' : \"DT\"}\n",
    "                try: \n",
    "                    cv_dt_data = load_cv_data(data_path,org,run,'DT')\n",
    "                except Exception as e:\n",
    "                    print(e) \n",
    "                    print(f'loading data problem with project {org} run {run} model DT')\n",
    "                    continue\n",
    "                new_row['complexity'] = compute_DT_model_complexity(cv_dt_data.best_estimator_)\n",
    "                new_row['max_depth'] =compute_DT_max_depth(cv_dt_data.best_estimator_)\n",
    "                new_row['nodes_count'] =nodes_count(cv_dt_data.best_estimator_)\n",
    "                results.append(new_row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def compute_GP_rule_complexity(rule,pondiration = {'Or' : 3, 'Xor' : 3, 'And' : 3, \"Terminal\" : 1}) : \n",
    "    nodes_labels = rule.split(';')\n",
    "    total_complexity = 0 \n",
    "    for node_label in nodes_labels:\n",
    "        if '[label = \"Xor\" ]' in node_label: \n",
    "             total_complexity += pondiration['Xor']\n",
    "\n",
    "        elif '[label = \"Or\" ]' in node_label: \n",
    "             total_complexity += pondiration['Or']\n",
    "        \n",
    "        elif 'And' in node_label: \n",
    "             total_complexity += pondiration['And']\n",
    "        \n",
    "        else : \n",
    "            total_complexity += pondiration['Terminal']\n",
    "    \n",
    "    return total_complexity\n",
    "        \n",
    "def max_depth_rule(rule) :\n",
    "    max_d = []\n",
    "    #dfs_depth base Code reference networkx\n",
    "    print('new_rule')\n",
    "    graphs = pydot.graph_from_dot_data('digraph {' + rule + ' }')\n",
    "    graph = graphs[0]\n",
    "    G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "    n_nodes = len(G)\n",
    "    depth = int(np.log2(n_nodes))\n",
    "    print('looking for max depth')\n",
    "    return depth\n",
    "\n",
    "def rule_nodes_count(rule) : \n",
    "    print('new_rule depth')\n",
    "    graphs = pydot.graph_from_dot_data('digraph {' + rule + ' }')\n",
    "    graph = graphs[0]\n",
    "    G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "\n",
    "    return len(G.nodes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multicr_rules = pd.read_csv('MULTICR_rules.csv')\n",
    "multicr_rules.loc[multicr_rules[\"project_name\"] == \"eclipse\",'project_name'] = 'Eclipse'\n",
    "multicr_selected_rules = FINAL_RESULT[FINAL_RESULT['algorithm'] == 'ibea'][['project_name','algorithm','fold','run','model_id']]\n",
    "multicr_selected_rules_complexities = pd.merge(multicr_selected_rules,multicr_rules,on=['project_name', 'algorithm', 'fold', 'run', 'model_id'],how='inner')\n",
    "print(len(multicr_selected_rules_complexities))\n",
    "print(multicr_selected_rules_complexities)\n",
    "multicr_selected_rules_complexities['complexity'] = multicr_selected_rules_complexities.apply(lambda row: compute_GP_rule_complexity(rule = str(row[\"rule\"])),axis=1)\n",
    "multicr_selected_rules_complexities['nodes_count'] = multicr_selected_rules_complexities.apply(lambda row: rule_nodes_count(rule = str(row[\"rule\"])),axis=1)\n",
    "multicr_selected_rules_complexities['max_depth'] = multicr_selected_rules_complexities.apply(lambda row: max_depth_rule(rule = str(row[\"rule\"])),axis=1)\n",
    "\n",
    "FINAL_RESULT['fold'] = FINAL_RESULT.apply(lambda row: int(row['file_id'].split('_')[1]),axis=1)\n",
    "print(multicr_selected_rules_complexities.head())\n",
    "DT_complexity=compute_dt_models_complexities(['Eclipse','Libreoffice','Gerrithub'],range(5), ML_MODELS_PATH)\n",
    "MULTICR_DT_COMPLEXITIES_ALL = pd.concat([DT_complexity[['project_name', 'fold', 'run', 'algorithm', 'complexity', 'max_depth', 'nodes_count']],\n",
    "                                          multicr_selected_rules_complexities[['project_name', 'fold', 'run', 'algorithm', 'complexity', 'max_depth', 'nodes_count']]\n",
    "                                                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b95fc8",
   "metadata": {},
   "source": [
    "## RQ5: Rules analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('.')\n",
    "import graphviz\n",
    "import networkx as nx\n",
    "import pydot\n",
    "from sympy import symbols\n",
    "from sympy import *\n",
    "from sympy.logic.boolalg import to_cnf\n",
    "import copy\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from explainer import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 5\n",
    "FEATURES = ['author_experience','author_merge_ratio', 'author_changes_per_week',\n",
    "       'author_merge_ratio_in_project', 'total_change_num',\n",
    "       'author_review_num', 'description_length', 'is_documentation',\n",
    "       'is_bug_fixing', 'is_feature', 'project_changes_per_week',\n",
    "       'project_merge_ratio', 'changes_per_author', 'num_of_reviewers',\n",
    "       'num_of_bot_reviewers', 'avg_reviewer_experience',\n",
    "       'avg_reviewer_review_count', 'lines_added', 'lines_deleted',\n",
    "       'files_added', 'files_deleted', 'files_modified', 'num_of_directory',\n",
    "       'modify_entropy', 'subsystem_num'\n",
    "            #, 'text_prob_ngram'\n",
    "           ]\n",
    "TARGET = 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf9591d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FEATURES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Motaz\\Desktop\\work\\replication packages\\multicr\\result analysis scripts\\results_processing.ipynb Cell 21\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Motaz/Desktop/work/replication%20packages/multicr/result%20analysis%20scripts/results_processing.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_features_data\u001b[39m(topn_models_per_run, studied_features\u001b[39m=\u001b[39m FEATURES) : \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Motaz/Desktop/work/replication%20packages/multicr/result%20analysis%20scripts/results_processing.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     file_ids \u001b[39m=\u001b[39m topn_models_per_run[\u001b[39m'\u001b[39m\u001b[39mfile_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Motaz/Desktop/work/replication%20packages/multicr/result%20analysis%20scripts/results_processing.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     runs \u001b[39m=\u001b[39m topn_models_per_run[\u001b[39m'\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FEATURES' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features_data(topn_models_per_run, studied_features= FEATURES) : \n",
    "    file_ids = topn_models_per_run['file_id'].unique()\n",
    "    runs = topn_models_per_run['run'].unique()\n",
    "    topn_models_per_run['Terminals'] = topn_models_per_run.apply(lambda row : extract_terminals(row['rule']),axis = 1 )\n",
    "    projects = topn_models_per_run['project_name'].unique()\n",
    "    print(projects)\n",
    "    occurence_per_projects = {project : {feature: 0 for feature in studied_features} for project in projects}\n",
    "    occurence_per_file_id = {file : {feature: 0 for feature in studied_features} for file in file_ids}\n",
    "    frequency_per_projects = {project : {feature: [] for feature in studied_features} for project in projects}\n",
    "    frequency_per_file_id = {file : {feature: [] for feature in studied_features} for file in file_ids}\n",
    "\n",
    "    for file_id in file_ids: \n",
    "        for run in runs :\n",
    "            selected_models = select_by_run(select_by_filenames(topn_models_per_run,[file_id]),[run])\n",
    "            \n",
    "            for index, row in selected_models.iterrows() :\n",
    "                run_occurencies = {feature : 0 for feature in studied_features} \n",
    "                terminals_list = ast.literal_eval(str(row['Terminals']))\n",
    "                for terminal in terminals_list: \n",
    "                    feature_name, op, threshold_value = parse_feature_name(terminal)\n",
    "                    frequency_per_projects[row['project_name']][feature_name].append((op, threshold_value))\n",
    "                    frequency_per_file_id[file_id][feature_name].append((op, threshold_value))\n",
    "                    if run_occurencies[feature_name] == 0 : \n",
    "                        run_occurencies[feature_name] = 1 \n",
    "                        occurence_per_projects[row['project_name']][feature_name] += 1 \n",
    "                        occurence_per_file_id[file_id][feature_name] += 1 \n",
    "                    else: \n",
    "                        print(file_id, row, row['model_id'], feature_name, 'already encounted')\n",
    "    return frequency_per_projects, frequency_per_file_id, occurence_per_projects, occurence_per_file_id\n",
    "\n",
    "def select_topn_per_run(df,rules_df,criteria = 'MCC_times_f1_M',topn=TOP_N): \n",
    "    file_ids = df['file_id'].unique()\n",
    "    print(file_ids)\n",
    "    print(rules_df['file_id'].unique())\n",
    "    runs = df['run'].unique()\n",
    "    train_data =  select_train_or_test(df,\"test\")\n",
    "    test_data =  select_train_or_test(df,\"test\")\n",
    "    all_selected_rules=[]\n",
    "    for file_id in file_ids: \n",
    "        for run in runs : \n",
    "            file_id_train = select_all_but_best(select_by_run(select_by_filenames(train_data,[file_id]),[run]))\n",
    "            file_id_test = select_all_but_best(select_by_run(select_by_filenames(test_data,[file_id]),[run]))\n",
    "            top_n_models = list(select_top_n(file_id_train,criteria=criteria, n=topn)['model_id'])\n",
    "            selected_rules = select_by_model(select_by_run(select_by_filenames(rules_df,[file_id]),[run]),top_n_models)\n",
    "            all_selected_rules.append(selected_rules)\n",
    "    return pd.concat(all_selected_rules)\n",
    "\n",
    "def select_top_n(data,criteria,n=TOP_N) : \n",
    "    data[\"selection_criteria\"] = data.apply(criteria,axis=1)\n",
    "    top_n_models = data.nlargest(n,['selection_criteria'])\n",
    "    data.drop(columns = [\"selection_criteria\"])\n",
    "    return top_n_models\n",
    "\n",
    "def extract_terminals(rule_str) : \n",
    "    graphs = pydot.graph_from_dot_data('digraph {' + rule_str + ' }')\n",
    "    graph = graphs[0]\n",
    "    G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "    terminals = []\n",
    "    for node in G.nodes: \n",
    "        if str(G._node[str(node)]['label'].replace('\"','')) in [\"Or\",\"Xor\",\"And\"]:\n",
    "            continue \n",
    "        terminals.append(str(G._node[str(node)]['label']).replace('\"',''))\n",
    "    return terminals \n",
    "\n",
    "def parse_feature_name(terminal) : \n",
    "    parsed_feature = terminal.split(' ')\n",
    "    return str(parsed_feature[0]), str(parsed_feature[1]), float(parsed_feature[2]) \n",
    "class RuleWarpper: \n",
    "    def __init__(self,rule_dot_str) -> None:\n",
    "        self.rule_dot_str = rule_dot_str\n",
    "        self.cleaned_dot_rule = RuleWarpper.simplfy_dot_rule(rule_dot_str)\n",
    "        #print(self.cleaned_dot_rule)\n",
    "        self.renamed_rule, self.rename_dict=RuleWarpper.rename_terminals_dot_rule(self.cleaned_dot_rule)\n",
    "        self.sympy_rule_str = RuleWarpper.from_dot_to_sympy(self.renamed_rule)\n",
    "        self.symbols_list = [symbol for symbol in self.rename_dict]\n",
    "        self.sympy_negatition_rule_str = '~ (' + self.sympy_rule_str +')'\n",
    "        #print(self.sympy_negatition_rule_str)\n",
    "        self.sympy_rule_sympified = to_dnf(self.sympy_rule_str,simplify=True,force=True)\n",
    "        print(self.sympy_rule_sympified)\n",
    "        self.sympy_negatition_rule_sympified = to_dnf(self.sympy_negatition_rule_str,simplify=True,force=True)\n",
    "        print(self.sympy_negatition_rule_sympified)\n",
    "        print(self.rename_dict)\n",
    "        #print(self.sympy_rule_str)\n",
    "        #print(self.sympy_rule_sympified)\n",
    "        #print(self.sympy_negatition_rule_sympified)\n",
    "    \n",
    "    def explain_instance(self,instance_to_explain): \n",
    "        rule_literals = str(self.sympy_rule_sympified).split('|')\n",
    "        prediction = None \n",
    "        explanation = '' \n",
    "        for literal in rule_literals: \n",
    "            literal_sympy = sympify(literal)\n",
    "            value = RuleWarpper.evaluate_sympy_rule(literal_sympy,self.rename_dict,instance_to_explain)\n",
    "            if value : \n",
    "                prediction = True\n",
    "                explanation += literal + ' '\n",
    "        \n",
    "        if len(explanation) > 0 : \n",
    "            return prediction, explanation\n",
    "\n",
    "        rule_literals = str(self.sympy_negatition_rule_sympified).split('|')\n",
    "        for literal in rule_literals: \n",
    "            literal_sympy = sympify(literal)\n",
    "            value = RuleWarpper.evaluate_sympy_rule(literal_sympy,self.rename_dict,instance_to_explain)\n",
    "            if value: \n",
    "                prediction = False\n",
    "                explanation += literal + ' '\n",
    "        \n",
    "        if len(explanation) > 0 : \n",
    "            return prediction, explanation\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_sympy_rule(sympy_rule,rename_dict,features_values) : \n",
    "        terminal_values = {variable: RuleWarpper.evaluate_terminal( rename_dict[variable],env=features_values) for variable in rename_dict}\n",
    "        terminals = symbols([var for var in rename_dict])\n",
    "        #print(sympy_rule.subs({var: terminal_values[str(var)] for var in terminals}))\n",
    "        return sympy_rule.subs({var: terminal_values[str(var)] for var in terminals})\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_terminal(terminal_expression, env) : \n",
    "        terminal_name, op, threshold_value = terminal_expression.split(' ')\n",
    "        if op == '=':\n",
    "            return env[terminal_name] == float(threshold_value)\n",
    "        \n",
    "        if op == '>=' : \n",
    "            return env[terminal_name] >= float(threshold_value)\n",
    "        \n",
    "        if op == '<=' : \n",
    "            return env[terminal_name] <= float(threshold_value)\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_dot_to_sympy(rule_dot) : \n",
    "        graphs = pydot.graph_from_dot_data('digraph {' + rule_dot + ' }')\n",
    "        graph = graphs[0]\n",
    "        G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "        is_visited = []\n",
    "        tree_root = [n for n,d in G.in_degree() if d==0][0]\n",
    "        return RuleWarpper.build_expression_from_graph(G, tree_root)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_expression_from_graph(G,node_id) : \n",
    "        label = G._node[str(node_id)]['label'].replace('\"','')\n",
    "        successors = G.successors(str(node_id))\n",
    "        if label == 'Xor' : \n",
    "            first_element = next(successors)\n",
    "            second_element = next(successors)\n",
    "            return f'({RuleWarpper.build_expression_from_graph(G, first_element)} & ~({RuleWarpper.build_expression_from_graph(G, second_element)})) | ({RuleWarpper.build_expression_from_graph(G, second_element)} & ~({RuleWarpper.build_expression_from_graph(G, first_element)})) '\n",
    "            #return f'({RuleWarpper.build_expression_from_graph(G,next(successors))} ^ {RuleWarpper.build_expression_from_graph(G,next(successors))})'\n",
    "        \n",
    "        if label == 'Or' :\n",
    "             return f'({RuleWarpper.build_expression_from_graph(G,next(successors))} | {RuleWarpper.build_expression_from_graph(G,next(successors))})'\n",
    "        \n",
    "        if label == 'And' :\n",
    "             return f'({RuleWarpper.build_expression_from_graph(G,next(successors))} & {RuleWarpper.build_expression_from_graph(G,next(successors))})'\n",
    "        \n",
    "        return str(label)\n",
    "            \n",
    "    @staticmethod\n",
    "    def rename_terminals_dot_rule(rule_str) : \n",
    "        terminal_rename = dict()\n",
    "        renamed_rule = copy.deepcopy(rule_str)\n",
    "        rule_terminals = list(set(RuleWarpper.extract_terminals_from_dot(rule_str)))\n",
    "        print(rule_terminals)\n",
    "        for terminal_index, terminal in enumerate(rule_terminals) : \n",
    "            terminal_rename['X'+str(terminal_index)] = terminal\n",
    "            renamed_rule = renamed_rule.replace(terminal, 'X'+str(terminal_index))\n",
    "        return renamed_rule, terminal_rename \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def extract_terminals_from_dot(rule_dot) : \n",
    "        graphs = pydot.graph_from_dot_data('digraph {' + rule_dot + ' }')\n",
    "        graph = graphs[0]\n",
    "        G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "        terminals = []\n",
    "        for node in G.nodes: \n",
    "            if str(G._node[str(node)]['label'].replace('\"','')) in [\"Or\",\"Xor\",\"And\",\"True\",\"False\"]:\n",
    "                continue \n",
    "            terminals.append(str(G._node[str(node)]['label']).replace('\"',''))\n",
    "        return terminals \n",
    "    \n",
    "    @staticmethod\n",
    "    def simplfy_dot_rule(rule_dot,non_negative_features = ['num_of_reviewers'], binary_features=['is_documentation','is_bug_fixing', 'is_feature']): \n",
    "        # setting negative values to simplified_rule = copy.deepcopy(rule_dot)for non_negative_feature in non_negative_features:\n",
    "        simplified_rule = copy.deepcopy(rule_dot)\n",
    "        for non_negative_feature in non_negative_features:\n",
    "            simplified_rule = simplified_rule.replace(f'{non_negative_feature} <= 0',f'{non_negative_feature} = 0')\n",
    "\n",
    "        for binary_feature in binary_features: \n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} >= 0', 'True')\n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} <= 0', f'{binary_feature} = 0')\n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} <= 1', 'True')\n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} >= 1', f'{binary_feature} = 1')\n",
    "        return simplified_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4702ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "multicr_rules = pd.read_csv('MULTICR_rules.csv')\n",
    "GP_DATA = prepare_GP_results(GP_RESULTS_PATH)\n",
    "print(GP_DATA['project_name'].unique())\n",
    "print(GP_DATA.columns)\n",
    "STUDIED_COLUMNS = [ 'project_name','file_id','fold','run','algorithm','model_id','train_or_test','MCC','f1_A','f1_M']\n",
    "multicr_data = GP_DATA[GP_DATA[\"algorithm\"] == 'ibea'][STUDIED_COLUMNS]\n",
    "multicr_rules = multicr_rules[multicr_rules['algorithm'] == 'ibea']\n",
    "multicr_rules.loc[multicr_rules[\"project_name\"] == \"eclipse\",'project_name'] = 'Eclipse'\n",
    "multicr_rules['file_id'] = multicr_rules.apply(lambda row: captilize_first_letter(row['file_id']),axis=1)\n",
    "multicr_data = add_metrics_product(multicr_data,['MCC','f1_M'])\n",
    "print(multicr_rules['project_name'].unique())\n",
    "print(multicr_data['project_name'].unique())\n",
    "selected_rules = select_topn_per_run(multicr_data, multicr_rules)\n",
    "freq_per_project, freq_per_file_id, occ_project, occ_file_id = extract_features_data(selected_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614de4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_rules = select_topn_per_run(multicr_data, multicr_rules)\n",
    "freq_per_project, freq_per_file_id, occ_project, occ_file_id = extract_features_data(selected_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36caac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleWarpper: \n",
    "    def __init__(self,rule_dot_str) -> None:\n",
    "        self.rule_dot_str = rule_dot_str\n",
    "        self.cleaned_dot_rule = RuleWarpper.simplfy_dot_rule(rule_dot_str)\n",
    "        #print(self.cleaned_dot_rule)\n",
    "        self.renamed_rule, self.rename_dict=RuleWarpper.rename_terminals_dot_rule(self.cleaned_dot_rule)\n",
    "        self.sympy_rule_str = RuleWarpper.from_dot_to_sympy(self.renamed_rule)\n",
    "        self.symbols_list = [symbol for symbol in self.rename_dict]\n",
    "        self.sympy_negatition_rule_str = '~ (' + self.sympy_rule_str +')'\n",
    "        #print(self.sympy_negatition_rule_str)\n",
    "        self.sympy_rule_sympified = to_dnf(self.sympy_rule_str,simplify=True,force=True)\n",
    "        print(self.sympy_rule_sympified)\n",
    "        self.sympy_negatition_rule_sympified = to_dnf(self.sympy_negatition_rule_str,simplify=True,force=True)\n",
    "        print(self.sympy_negatition_rule_sympified)\n",
    "        print(self.rename_dict)\n",
    "        #print(self.sympy_rule_str)\n",
    "        #print(self.sympy_rule_sympified)\n",
    "        #print(self.sympy_negatition_rule_sympified)\n",
    "    \n",
    "    def explain_instance(self,instance_to_explain): \n",
    "        rule_literals = str(self.sympy_rule_sympified).split('|')\n",
    "        prediction = None \n",
    "        explanation = '' \n",
    "        for literal in rule_literals: \n",
    "            literal_sympy = sympify(literal)\n",
    "            value = RuleWarpper.evaluate_sympy_rule(literal_sympy,self.rename_dict,instance_to_explain)\n",
    "            if value : \n",
    "                prediction = True\n",
    "                explanation += literal + ' '\n",
    "        \n",
    "        if len(explanation) > 0 : \n",
    "            return prediction, explanation\n",
    "\n",
    "        rule_literals = str(self.sympy_negatition_rule_sympified).split('|')\n",
    "        for literal in rule_literals: \n",
    "            literal_sympy = sympify(literal)\n",
    "            value = RuleWarpper.evaluate_sympy_rule(literal_sympy,self.rename_dict,instance_to_explain)\n",
    "            if value: \n",
    "                prediction = False\n",
    "                explanation += literal + ' '\n",
    "        \n",
    "        if len(explanation) > 0 : \n",
    "            return prediction, explanation\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_sympy_rule(sympy_rule,rename_dict,features_values) : \n",
    "        terminal_values = {variable: RuleWarpper.evaluate_terminal( rename_dict[variable],env=features_values) for variable in rename_dict}\n",
    "        terminals = symbols([var for var in rename_dict])\n",
    "        #print(sympy_rule.subs({var: terminal_values[str(var)] for var in terminals}))\n",
    "        return sympy_rule.subs({var: terminal_values[str(var)] for var in terminals})\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_terminal(terminal_expression, env) : \n",
    "        terminal_name, op, threshold_value = terminal_expression.split(' ')\n",
    "        if op == '=':\n",
    "            return env[terminal_name] == float(threshold_value)\n",
    "        \n",
    "        if op == '>=' : \n",
    "            return env[terminal_name] >= float(threshold_value)\n",
    "        \n",
    "        if op == '<=' : \n",
    "            return env[terminal_name] <= float(threshold_value)\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_dot_to_sympy(rule_dot) : \n",
    "        graphs = pydot.graph_from_dot_data('digraph {' + rule_dot + ' }')\n",
    "        graph = graphs[0]\n",
    "        G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "        is_visited = []\n",
    "        tree_root = [n for n,d in G.in_degree() if d==0][0]\n",
    "        return RuleWarpper.build_expression_from_graph(G, tree_root)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_expression_from_graph(G,node_id) : \n",
    "        label = G._node[str(node_id)]['label'].replace('\"','')\n",
    "        successors = G.successors(str(node_id))\n",
    "        if label == 'Xor' : \n",
    "            first_element = next(successors)\n",
    "            second_element = next(successors)\n",
    "            return f'({RuleWarpper.build_expression_from_graph(G, first_element)} & ~({RuleWarpper.build_expression_from_graph(G, second_element)})) | ({RuleWarpper.build_expression_from_graph(G, second_element)} & ~({RuleWarpper.build_expression_from_graph(G, first_element)})) '\n",
    "            #return f'({RuleWarpper.build_expression_from_graph(G,next(successors))} ^ {RuleWarpper.build_expression_from_graph(G,next(successors))})'\n",
    "        \n",
    "        if label == 'Or' :\n",
    "             return f'({RuleWarpper.build_expression_from_graph(G,next(successors))} | {RuleWarpper.build_expression_from_graph(G,next(successors))})'\n",
    "        \n",
    "        if label == 'And' :\n",
    "             return f'({RuleWarpper.build_expression_from_graph(G,next(successors))} & {RuleWarpper.build_expression_from_graph(G,next(successors))})'\n",
    "        \n",
    "        return str(label)\n",
    "            \n",
    "    @staticmethod\n",
    "    def rename_terminals_dot_rule(rule_str) : \n",
    "        terminal_rename = dict()\n",
    "        renamed_rule = copy.deepcopy(rule_str)\n",
    "        rule_terminals = list(set(RuleWarpper.extract_terminals_from_dot(rule_str)))\n",
    "        print(rule_terminals)\n",
    "        for terminal_index, terminal in enumerate(rule_terminals) : \n",
    "            terminal_rename['X'+str(terminal_index)] = terminal\n",
    "            renamed_rule = renamed_rule.replace(terminal, 'X'+str(terminal_index))\n",
    "        return renamed_rule, terminal_rename \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def extract_terminals_from_dot(rule_dot) : \n",
    "        graphs = pydot.graph_from_dot_data('digraph {' + rule_dot + ' }')\n",
    "        graph = graphs[0]\n",
    "        G = nx.DiGraph(nx.nx_pydot.from_pydot(graph))\n",
    "        terminals = []\n",
    "        for node in G.nodes: \n",
    "            if str(G._node[str(node)]['label'].replace('\"','')) in [\"Or\",\"Xor\",\"And\",\"True\",\"False\"]:\n",
    "                continue \n",
    "            terminals.append(str(G._node[str(node)]['label']).replace('\"',''))\n",
    "        return terminals \n",
    "    \n",
    "    @staticmethod\n",
    "    def simplfy_dot_rule(rule_dot,non_negative_features = ['num_of_reviewers'], binary_features=['is_documentation','is_bug_fixing', 'is_feature']): \n",
    "        # setting negative values to simplified_rule = copy.deepcopy(rule_dot)for non_negative_feature in non_negative_features:\n",
    "        simplified_rule = copy.deepcopy(rule_dot)\n",
    "        for non_negative_feature in non_negative_features:\n",
    "            simplified_rule = simplified_rule.replace(f'{non_negative_feature} <= 0',f'{non_negative_feature} = 0')\n",
    "\n",
    "        for binary_feature in binary_features: \n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} >= 0', 'True')\n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} <= 0', f'{binary_feature} = 0')\n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} <= 1', 'True')\n",
    "            simplified_rule = simplified_rule.replace(f'{binary_feature} >= 1', f'{binary_feature} = 1')\n",
    "        return simplified_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b33b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main \n",
    "def prepare_data_GP(df) : \n",
    "\tclean_df = df.copy()\n",
    "\tboolean_cols = ['is_bug_fixing','is_documentation','is_feature'] \n",
    "\tclean_df = clean_df.drop(columns = ['project','change_id','created','subject'])\n",
    "\tclean_df['status'] = 1 - clean_df['status'] \n",
    "\tfor col in boolean_cols: \n",
    "\t\tclean_df[col] = clean_df[col].astype(int) \n",
    "\t\n",
    "\treturn clean_df\n",
    "\n",
    "PROJECT = 'Eclipse'\n",
    "DATA_PATH = './data'\n",
    "folds = 11 \n",
    "fold = 10\n",
    "project_data = pd.read_csv(os.path.join(DATA_PATH,PROJECT+'.csv'))\n",
    "train_size = project_data.shape[0] * fold // folds\n",
    "test_size = min(project_data.shape[0] * (fold + 1) // folds, project_data.shape[0])\n",
    "\n",
    "x_train, y_train = project_data.loc[:train_size - 1, FEATURES], project_data.loc[:train_size - 1, TARGET]\n",
    "x_test, y_test = project_data.loc[train_size:test_size - 1, FEATURES], project_data.loc[train_size:test_size - 1, TARGET]\n",
    "\t\t\t\n",
    "clean_df = prepare_data_GP(project_data) \n",
    "train_df = clean_df.iloc[:train_size - 1]\n",
    "test_df = clean_df.iloc[train_size:test_size - 1]\n",
    "SAMPLE_RULE = ' 1 [label = \"Xor\" ] 1->2; 2 [label = \"Or\" ] 2->4; 4 [label = \"Or\" ] 4->8; 8 [label = \"lines_deleted >= 1318157\" ] 4->9; 9 [label = \"author_merge_ratio_in_project <= 0.63\" ] 2->5; 5 [label = \"Or\" ] 5->10; 10 [label = \"lines_added >= 7547947\" ] 5->11; 11 [label = \"changes_per_author <= 58.35\" ] 1->3; 3 [label = \"And\" ] 3->6; 6 [label = \"And\" ] 6->12; 12 [label = \"files_modified <= 10294\" ] 6->13; 13 [label = \"num_of_reviewers >= 1\" ] 3->7; 7 [label = \"Or\" ] 7->14; 14 [label = \"changes_per_author <= 57.19\" ] 7->15; 15 [label = \"total_change_num >= 460\" ]'\n",
    "myrule = RuleWarpper(SAMPLE_RULE)\n",
    "predictions = []\n",
    "explanations = []\n",
    "for index, row in test_df.iterrows() : \n",
    "\tprediction, explanation = myrule.explain_instance(row)\n",
    "\tpredictions.append(int(prediction))\n",
    "\texplanations.append(explanation)\n",
    "\n",
    "print(matthews_corrcoef(test_df[TARGET],predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ff468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime \n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def explain_instance(explainer, data, instances_idx, model, num_features=10, top_labels=1) :\n",
    "    results = {}\n",
    "    for instance_index in instances_idx : \n",
    "        instance = data.iloc[i]\n",
    "        explanation = explainer.explain_instance(instance, model.predict_proba, num_features=num_features, top_labels=top_labels)\n",
    "        results[i] = explanation\n",
    "    return results\n",
    "PROJECTS = ['Eclipse', 'Libreoffice', 'Gerrithub']\n",
    "PROJECT = 'Eclipse'\n",
    "DATA_PATH = './data'\n",
    "MODEL_NAME = 'RF'\n",
    "MODEL_PATH = f'C:/Users/Moataz/Desktop/work/code_review_delay_prediction/islam_data/Results/{PROJECT}/{PROJECT}/{MODEL_NAME}/{PROJECT}_{MODEL_NAME}_cross_val_run_0.pk'\n",
    "PROJECT_CV_DATA = pickle.load(open(MODEL_PATH,'rb'))\n",
    "MODEL = PROJECT_CV_DATA.best_estimator_\n",
    "NB_PROCESS = 10 \n",
    "FOLDS = 11 \n",
    "fold = 10\n",
    "for porject in PROJECTS:\n",
    "    project_data = pd.read_csv(os.path.join(DATA_PATH,porject+'.csv'))\n",
    "    for fold in range(1,FOLDS): \n",
    "        train_size = project_data.shape[0] * fold // FOLDS\n",
    "        test_size = min(project_data.shape[0] * (fold + 1) // FOLDS, project_data.shape[0])\n",
    "        scaler = StandardScaler()\n",
    "        x_train, y_train = project_data.loc[:train_size - 1, FEATURES], project_data.loc[:train_size - 1, TARGET]\n",
    "        x_test, y_test = project_data.loc[train_size:test_size - 1, FEATURES], project_data.loc[train_size:test_size - 1, TARGET]\n",
    "        x_train=scaler.fit_transform(x_train)\n",
    "        y_train=scaler.transform(x_test)\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(x_train, feature_names=MODEL.feature_names_in_, class_names=[0, 1], discretize_continuous=True)\n",
    "        new_row = {\n",
    "            'Explained_model' :MODEL_NAME, \n",
    "            'Iteration' : fold - 1, \n",
    "        }\n",
    "        \n",
    "        idex_chunks = chunks(list(range(len(x_test))),NB_PROCESS) \n",
    "        fold_explanations = {}\n",
    "        with Pool(NB_PROCESS) as p:\n",
    "            arguments = [(explainer,x_test, chunk ,MODEL,10,1) for chunk in idex_chunks]\n",
    "            print(p.map(explain_instances,arguments))\n",
    "            fold_explanations.update(p.map(explain_instances,*arguments))\n",
    "        \n",
    "        explanations = [None for _ in range(len(x_test))]\n",
    "        fold_lime_preds = [None for _ in range(len(x_test))]\n",
    "        for index, exp in fold_explanations.items() : \n",
    "            explanations[index] = exp\n",
    "            fold_lime_preds[index] = exp.top_labels[0]\n",
    "        \n",
    "\n",
    "        print(matthews_corrcoef(y_test,fold_lime_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.BaseDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b2bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matthews_corrcoef(y_test,lime_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train, feature_names=MODEL.feature_names_in_, class_names=[0, 1], discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b758bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, x_test.shape[0])\n",
    "print(i)\n",
    "exp = explainer.explain_instance(x_test.iloc[i], MODEL.predict_proba, num_features=20, top_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd742a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf99d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(explanations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5231e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RULE = '''\n",
    "  1 [label = \"Or\"];\n",
    "    3 [label = \"Or\"];\n",
    "    2 [label = \"And\"];\n",
    "    4 [label = \"And\"];\n",
    "    5 [label = \"And\"];\n",
    "    6 [label = \"Or\"];\n",
    "    7 [label = \"Or\"];\n",
    "    8 [label = \"num_of_reviewers <= 1\"]\n",
    "    9 [label = \"num_of_directory < 54\"]\n",
    "    10 [label = \"description_length < 2\"]\n",
    "    11 [label= \"num_of_directory >= 2\"]\n",
    "    12 [label = \"description_length < 2\"]\n",
    "    13 [label = \"total_change_num < 43\"]\n",
    "    14 [label = \"lines_added > 221\"]\n",
    "    15 [label  = \"is_documentation = 0\"]\n",
    "    1 -> 2\n",
    "    1->3\n",
    "    3 -> 4\n",
    "    3 -> 5\n",
    "    2 -> 6 \n",
    "    2 -> 7\n",
    "    4->8\n",
    "    4->9\n",
    "    5->10\n",
    "    5->11\n",
    "    6->12\n",
    "    6->13\n",
    "    7->14\n",
    "    7->15\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrule = RuleWarpper(SAMPLE_RULE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for literal in str(myrule.sympy_rule_sympified).split('|'): \n",
    "    print(sympify(literal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(symp_symbols[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ea870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "069d64f6",
   "metadata": {},
   "source": [
    "## Discussion: Performance against the other MOEAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.indicators.gd import GD\n",
    "from pymoo.indicators.hv import HV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_indicator(df, train_or_test = \"test\") : \n",
    "    files_ids = df['file_id'].unique() \n",
    "    runs = df['run'].unique() \n",
    "    algorithms  = df['algorithm'].unique() \n",
    "    results = []\n",
    "    for file_id in files_ids : \n",
    "        for run in runs: \n",
    "            for algo in algorithms: \n",
    "                print('file:',file_id)\n",
    "                print('run:',run)\n",
    "                print('algo:',algo)\n",
    "                data = select_all_but_best(select_train_or_test(select_algorithms(select_by_run(select_by_filenames(df,[file_id]),[run]),[algo]),train_or_test))\n",
    "                print(data)\n",
    "                objs = data[['recall_A','recall_M']]\n",
    "                hv = compute_hv(objs*-1)\n",
    "                gd = compute_gd(objs)\n",
    "                new_row = {\n",
    "                    'project_name' : data['project_name'].unique()[0],\n",
    "                    'file_id': file_id,\n",
    "                    'run': run,\n",
    "                    'algorithm': algo,\n",
    "                    'HV': hv,\n",
    "                    'GD' : gd\n",
    "                }\n",
    "                results.append(new_row)\n",
    "    return pd.DataFrame(results)\n",
    "def compute_hv(data,ref_point = np.array([0,0])) : \n",
    "    hv = HV(ref_point=ref_point)\n",
    "    print(data)\n",
    "    return hv(np.array(data.values.tolist()))\n",
    "\n",
    "def compute_gd(data,pf =np.array([[1.0,1.0]])) : \n",
    "    gd = GD(pf)\n",
    "    return gd(np.array(data.values.tolist()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ab614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "GP_DATA = prepare_GP_results(GP_RESULTS_PATH)\n",
    "print(GP_DATA['file_id'].unique())\n",
    "STUDIED_COLUMNS = [ 'project_name','file_id','fold','run','algorithm','model_id','train_or_test','MCC','f1_A','f1_M','recall_A','recall_M']\n",
    "FINAL_DATA = pd.concat([\n",
    "    GP_DATA[STUDIED_COLUMNS]\n",
    "])\n",
    "FINAL_DATA = add_metrics_product(FINAL_DATA,['MCC','f1_M'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f0b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification performance data\n",
    "FINAL_RESULT[(FINAL_RESULT[\"algorithm\"].isin(['ibea','nsga2','nsga3','spea2']))].to_csv('multicr_VS_other_moeas.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = compute_performance_indicator(FINAL_DATA,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_DATA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc571fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators.to_csv('all_moeas_indicators_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ece991",
   "metadata": {},
   "source": [
    "## Discussion: Concept drift validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals\n",
    "GP_CONCEPT_DRIFT_RESULTS = 'C:/Users/Motaz/Desktop/work/code_review_delay_prediction/early_abondon_prediction/GP_summeries/GP_RESULTS/concept_drift'\n",
    "ML_CONCEPT_DRIFT_RESULTS = 'C:/Users/Motaz/Desktop/work/code_review_delay_prediction/early_abondon_prediction/GP_summeries/ML_RESULTS/concept_drift'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_concept_drift_ML_DATA(ML_DATA_PATH) : \n",
    "    all_data = pd.concat([\n",
    "        pd.read_csv(os.path.join(ML_DATA_PATH,'Eclipse_concept_drift_all_models.csv')),\n",
    "        pd.read_csv(os.path.join(ML_DATA_PATH,'Gerrithub_concept_drift_all_models.csv')),\n",
    "        pd.read_csv(os.path.join(ML_DATA_PATH,'Libreoffice_concept_drift_all_models.csv'))\n",
    "        ])\n",
    "    final_ml_data = process_ml_concept_drift_data(all_data)\n",
    "    return final_ml_data\n",
    "\n",
    "def prepare_concept_drift_GP_DATA(GP_DATA_PATH) : \n",
    "    all_data = pd.read_csv(GP_DATA_PATH)\n",
    "    all_data = all_data.rename(columns={\"tnr\": \"recall_M\", \"tpr\": \"recall_A\",'f1': 'f1_A','precision':\"precision_A\"})\n",
    "    all_data = add_metric(all_data,'precision_M',precision_M)\n",
    "    all_data = add_metric(all_data,'f1_M',f1_M)\n",
    "    all_data = add_metrics_product(all_data,['MCC','f1_M'])\n",
    "    all_data['is_new_or_old'] = all_data.apply(lambda row:  'new' if 'new' in row['algorithm'] else 'old' ,axis=1)\n",
    "    return all_data\n",
    "\n",
    "def process_ml_concept_drift_data(ml_data) : \n",
    "    result = ml_data.copy()\n",
    "    result = result.rename(columns={\"recall_m\": \"recall_M\", \"recall_a\": \"recall_A\",\n",
    "                                    'f1_score_a': 'f1_A','precision_a':\"precision_A\",\n",
    "                                    'f1_score_m' : 'f1_M','precision_m' : 'precision_M',\n",
    "                                    'project': 'project_name'\n",
    "                                   })\n",
    "    result[\"fold\"] = result['fold'] - 1 \n",
    "    result['file_id'] = result.apply(lambda row: extract_file_name_concept_drift(row) ,axis=1)\n",
    "    result['is_new_or_old'] = result.apply(lambda row:  'new' if 'new' in row['algorithm'] else 'old' ,axis=1)\n",
    "    return result \n",
    "\n",
    "def extract_file_name_concept_drift(row) : \n",
    "    file_name = row['project_name'] + '_'\n",
    "    if 'new' in row['algorithm'] : \n",
    "        file_name+= 'new_' \n",
    "    else :\n",
    "        file_name+= 'old_'\n",
    "    file_name += str(row['fold'])\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "GP_CONCEPT_DRIFT_DATA = prepare_concept_drift_GP_DATA(os.path.join(GP_CONCEPT_DRIFT_RESULTS,\"multicr_concept_drift.csv\"))\n",
    "ML_CONCEPT_DRIFT_DATA = prepare_concept_drift_ML_DATA(ML_CONCEPT_DRIFT_RESULTS)\n",
    "print(GP_CONCEPT_DRIFT_DATA['file_id'].unique())\n",
    "print(ML_CONCEPT_DRIFT_DATA['file_id'].unique())\n",
    "STUDIED_COLUMNS_CONCEPT_DRIFT = [ 'project_name','file_id','fold','run','algorithm','model_id','train_or_test','MCC','f1_A','f1_M','is_new_or_old']\n",
    "FINAL_DATA_CONCEPT_DRIFT = pd.concat([\n",
    "    GP_CONCEPT_DRIFT_DATA[STUDIED_COLUMNS_CONCEPT_DRIFT], ML_CONCEPT_DRIFT_DATA[STUDIED_COLUMNS_CONCEPT_DRIFT]\n",
    "])\n",
    "FINAL_DATA_CONCEPT_DRIFT = add_metrics_product(FINAL_DATA_CONCEPT_DRIFT,['MCC','f1_M'])\n",
    "FINAL_RESULT_CONCEPT_DRIFT = apply_selection_criteria(FINAL_DATA_CONCEPT_DRIFT,'MCC_times_f1_M',metrics = ['project_name','MCC','f1_M','f1_A','is_new_or_old'])\n",
    "FINAL_RESULT_CONCEPT_DRIFT.to_csv(\"GP_VS_ML_CONCEPT_DRIFT.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESULT_CONCEPT_DRIFT.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESULT_CONCEPT_DRIFT[500:].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dab379",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESULT_CONCEPT_DRIFT.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90289343",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESULT_CONCEPT_DRIFT['algorithm'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409355c",
   "metadata": {},
   "source": [
    "## Descussion: Bias toward new developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#globals\n",
    "GP_NEW_DEVELOPERS_RESULTS = 'C:/Users/Moataz/Desktop/work/code_review_delay_prediction/early_abondon_prediction/GP_summeries/GP_RESULTS/new_developers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b053556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_GP_new_developers_data_results(GP_DATA_PATH) : \n",
    "    all_GP_data = pd.concat([\n",
    "        pd.read_csv(os.path.join(GP_DATA_PATH,'new_developers_multicr.csv')),\n",
    "    ])\n",
    "    all_GP_data = all_GP_data.rename(columns={\"tnr\": \"recall_M\", \"tpr\": \"recall_A\",'f1': 'f1_A','precision':\"precision_A\"})\n",
    "    all_GP_data = add_metric(all_GP_data,'precision_M',precision_M)\n",
    "    all_GP_data = add_metric(all_GP_data,'f1_M',f1_M)\n",
    "    all_GP_data.loc[all_GP_data[\"project_name\"] == \"eclipse\",'project_name'] = 'Eclipse'\n",
    "    all_GP_data['file_id'] = all_GP_data.apply(lambda row: captilize_first_letter(row['project_name']) + '_' + str(row['fold'] ),axis=1)\n",
    "    all_GP_data = select_all_but_best(all_GP_data)\n",
    "    return all_GP_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "GP_DATA_NEW_DEVELOPER = prepare_GP_new_developers_data_results(GP_NEW_DEVELOPERS_RESULTS)\n",
    "STUDIED_COLUMNS = [ 'project_name','file_id','fold','run','algorithm','model_id','train_or_test','MCC','f1_A','f1_M']\n",
    "FINAL_DATA_NEW_DEVELOPER = pd.concat([\n",
    "    GP_DATA_NEW_DEVELOPER[STUDIED_COLUMNS]\n",
    "])\n",
    "FINAL_DATA_NEW_DEVELOPER = add_metrics_product(FINAL_DATA_NEW_DEVELOPER,['MCC','f1_M'])\n",
    "FINAL_RESULTS_NEW_DEVELOPER = apply_selection_criteria(FINAL_DATA_NEW_DEVELOPER,'MCC_times_f1_M',metrics = ['project_name','MCC','f1_M','f1_A'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4718f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESULTS_NEW_DEVELOPER.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ee784",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_RESULTS_NEW_DEVELOPER.to_csv('NEW_DEVELOPERS_RESULTS.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'Gerrithub'\n",
    "metric = 'f1_A'\n",
    "FINAL_RESULTS_NEW_DEVELOPER[(FINAL_RESULTS_NEW_DEVELOPER['project_name'] == project) & (FINAL_RESULTS_NEW_DEVELOPER['algorithm'] == 'ibea')][metric].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f6993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('code_review_delay_prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "accae3a3467c531b7ffc4ecf0116144a9eba798e4a8d63e800cbc9fb7642da16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
